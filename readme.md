# Awesome affordable methods for LLM reasoning in engineering

This is a collection of research papers & blogs for affordable methods capable of enhancing reasoning ability of LLM .

And the repository will be continuously updated to track the frontier of LLM Reasoning.

## Papers
* 18 Nov 2024 [ReST-MCTSâˆ—: LLM Self-Training via Process Reward
Guided Tree Search](https://keg.cs.tsinghua.edu.cn/jietang/publications/NeurIPS24-Zhang-et-al-ReST-MCTS.pdf#:~:text=traces%20as%20well%20as%20per-step%20value%20to%20train,is%20able%20to%20infer%20the%20correct%20process%20reward)
This work use mcts to enhance the capability of reasoning. It is remarkable that in their github repo, there is only small amount of examples. It may indicate that such methods only require small amount of data.

* 5 Nov 2024 [Large Language Models Orchestrating Structured Reasoning Achieve Kaggle Grandmaster Leve](https://arxiv.org/pdf/2411.03562)
In this paper, LLM agent shows remarkable performance that achieving Kaggle Grandmaster in kaggle competition. It introduces a method to enhance the performance of LLM agent by utilizing the historical data.

* 12 Oct 2024 [OpenR: An Open Source Framework for Advanced Reasoning with Large Language Models](https://arxiv.org/abs/2410.09671)
This work release a light, integrated framework to facilitate the RL-based fine-tune of LLM.

* 4 Oct 2024 [Training Language Models to Self-Correct via Reinforcement Learning](https://arxiv.org/abs/2409.12917)

* 18 Mar 2024 [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629)

* 26 Jun 2024 [STEP-DPO: STEP-WISE PREFERENCE OPTIMIZATION FOR LONG-CHAIN REASONING OF LLMS](https://arxiv.org/abs/2406.18629)
By lots of experiments, this work draw a conclusion that training a LLM with step by step data could significantly enhance the ability of reasoning. They mention that step-DPO could even enhance the performance of LLM after RLHF, which indicates that step-DPO could complement RLHF effectively.

* 17 Jun 2024 [Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning](https://arxiv.org/pdf/2405.00451) [![](https://img.shields.io/badge/github-repo-blue)](https://github.com/YuxiXie/MCTS-DPO)

## Blogs 

## Benchmark of open-sourced small language models (SLM)
### syslm

### livebench